{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T03:36:54.208963Z",
     "start_time": "2025-12-18T03:36:51.645063Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 50 simulations in batches of 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# CONFIG\n",
    "BATCH_SIZE = 50  # Process 10 simulations at once (Increase if GPU usage is still low)\n",
    "NUM_BATCHES = 1  # 5 batches * 10 per batch = 50 total simulations\n",
    "TARGET_SET = {\"Christmas\", \"Peace\", \"Tariff\"}\n",
    "\n",
    "# SETUP\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./trump_model_v1\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./trump_model_v1\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Fix padding for generation\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def run_batched_simulation(prompt):\n",
    "    total_hits = 0\n",
    "    total_sims = BATCH_SIZE * NUM_BATCHES\n",
    "    \n",
    "    print(f\"Running {total_sims} simulations in batches of {BATCH_SIZE}...\")\n",
    "    \n",
    "    # Prepare the batch inputs ONCE\n",
    "    # We simply repeat the prompt list N times\n",
    "    batch_inputs = tokenizer([prompt] * BATCH_SIZE, return_tensors=\"pt\", padding=True, return_attention_mask=True).to(device)\n",
    "    \n",
    "    for i in range(NUM_BATCHES):\n",
    "        # Generate 10 futures at the exact same time\n",
    "        outputs = model.generate(\n",
    "            input_ids=batch_inputs['input_ids'],\n",
    "            attention_mask=batch_inputs['attention_mask'],\n",
    "            max_new_tokens=5000, \n",
    "            do_sample=True, \n",
    "            temperature=0.85, \n",
    "            top_k=50\n",
    "        )\n",
    "        \n",
    "        # Check results for this batch\n",
    "        decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        for text in decoded_batch:\n",
    "            # Only check the NEW part (slice off the prompt)\n",
    "            new_content = text[len(prompt):].lower()\n",
    "            if any(t.lower() in new_content for t in TARGET_SET):\n",
    "                total_hits += 1\n",
    "                \n",
    "        print(f\"Batch {i+1}/{NUM_BATCHES} complete.\")\n",
    "\n",
    "    probability = total_hits / total_sims\n",
    "    return probability\n",
    "\n",
    "# --- RUN IT ---\n",
    "prompt = \"\"\"[MODE: SPEECH] [TRUMP]: Well, thank you very much. Nice place. I guess you've mostly been here, but you like it a lot better with Trump than you like it with Biden. That I can tell you. That's because you're smart. Well, I'm thrilled to welcome so many good friends to the White House as we celebrate the third night of Hanukkah. Third night. Time, time flies. Let me take a moment to send the love and prayers to our entire nation, to the people of Australia, and especially all those affected by the horrific and anti-Semitic terrorist attack, and that's exactly what it is, anti-Semitic.\"\"\"\n",
    "prob = run_batched_simulation(prompt)\n",
    "print(f\"Probability: {prob:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
